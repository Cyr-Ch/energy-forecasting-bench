# PatchTST model configuration
# Based on official implementation: https://github.com/yuqinie98/PatchTST
# Paper: "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers" (ICLR 2023)

model: patchtst

# Patch parameters
patch_len: 16  # Length of each patch (PatchTST/64 uses 16, PatchTST/42 uses 42)
stride: 8      # Stride for patch creation

# Transformer parameters
n_layers: 3    # Number of transformer encoder layers
n_heads: 8     # Number of attention heads
d_model: 512   # Model dimension
d_ff: 2048     # Feed-forward dimension (typically 4 * d_model)
dropout: 0.1   # Dropout rate
activation: gelu  # Activation function ('gelu' or 'relu')

# RevIN parameters
revin: true    # Whether to use Reversible Instance Normalization
affine: true   # Whether to use affine parameters in RevIN

# Head parameters
head_dropout: 0.0  # Dropout rate in prediction head

# Training parameters
optimizer: adamw
lr: 1e-3
weight_decay: 1e-4
scheduler: cosine
warmup_steps: 400
